{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#import nltk\n",
    "#nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import passports\n",
    "passPort = pd.read_excel('\\\\\\\\ad.ing.net\\\\WPS\\\\NL\\\\P\\\\GD\\\\012223\\\\01 MARKETING INTELLIGENCE\\\\AA_Projects\\\\201808_passportNPL_Analysis\\\\passport.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and filter purpose and success\n",
    "purpose = passPort[['Purpose']]\n",
    "purpose = purpose[~purpose['Purpose'].isnull()]\n",
    "success = passPort[['Success']]\n",
    "success = success[~success['Success'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing dataset\n",
    "trainPurpose = purpose\n",
    "trainSuccess = success\n",
    "\n",
    "#create all lowercase\n",
    "trainPurpose['Purpose'] = trainPurpose['Purpose'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "trainSuccess['Success'] = trainSuccess['Success'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "#remove punctuation\n",
    "trainPurpose['Purpose'] = trainPurpose['Purpose'].str.replace('[^\\w\\s]','')\n",
    "trainSuccess['Success'] = trainSuccess['Success'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#remove stopwords/commonly occurring words\n",
    "stop = stopwords.words('english')\n",
    "trainPurpose['Purpose'] = trainPurpose['Purpose'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "trainSuccess['Success'] = trainSuccess['Success'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Spelling correction\n",
    "#from textblob import TextBlob\n",
    "#train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "#lemmatization [convert words to its stem]\n",
    "trainPurpose['Purpose'] = trainPurpose['Purpose'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "trainSuccess['Success'] = trainSuccess['Success'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# Frequently used words\n",
    "freqT10_purpose = pd.Series(' '.join(trainPurpose['Purpose']).split()).value_counts()[:10]\n",
    "freqT10_success = pd.Series(' '.join(trainSuccess['Success']).split()).value_counts()[:10]\n",
    "#remove these words\n",
    "#freqT10_purpose = list(freqT10_purpose.index)\n",
    "#freqT10_success = list(freqT10_success.index)\n",
    "#trainPurpose['Purpose'] = trainPurpose['Purpose'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqT10_purpose))\n",
    "#trainSuccess['Success'] = trainSuccess['Success'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqT10_success))\n",
    "\n",
    "# Rare used words\n",
    "freqTmin10_purpose = pd.Series(' '.join(trainPurpose['Purpose']).split()).value_counts()[-10:]\n",
    "freqTmin10_success = pd.Series(' '.join(trainSuccess['Success']).split()).value_counts()[-10:]\n",
    "#remove these words\n",
    "#freqTmin10_purpose = list(freqTmin10_purpose.index)\n",
    "#freqTmin10_success = list(freqTmin10_success.index)\n",
    "#trainPurpose['Purpose'] = trainPurpose['Purpose'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTmin10_purpose))\n",
    "#trainSuccess['Success'] = trainSuccess['Success'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTmin10_success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create N grams purpose\n",
    "df = trainPurpose[['Purpose']]\n",
    "\n",
    "for run in range(1,6) :\n",
    "\n",
    "    word_vectorizer = CountVectorizer(ngram_range=(run,run+1), analyzer='word')\n",
    "\n",
    "    sparse_matrix = word_vectorizer.fit_transform(df['Purpose'])\n",
    "\n",
    "    frequencies = sum(sparse_matrix).toarray()[0]\n",
    "\n",
    "    dfOut = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "\n",
    "    dfOut = dfOut.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    dfOut = dfOut.reset_index()\n",
    "    if run == 1:\n",
    "        nGramResultsPurpose = dfOut.iloc[[0,1,2,3,4],[0,1]]\n",
    "    else:    \n",
    "        nGramResultsPurpose = pd.concat([nGramResultsPurpose, dfOut.iloc[[0,1,2,3,4],[0,1]]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create N grams purpose\n",
    "df = trainSuccess[['Success']]\n",
    "\n",
    "for run in range(1,6) :\n",
    "\n",
    "    word_vectorizer = CountVectorizer(ngram_range=(run,run+1), analyzer='word')\n",
    "\n",
    "    sparse_matrix = word_vectorizer.fit_transform(df['Success'])\n",
    "\n",
    "    frequencies = sum(sparse_matrix).toarray()[0]\n",
    "\n",
    "    dfOut = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "\n",
    "    dfOut = dfOut.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    dfOut = dfOut.reset_index()\n",
    "    if run == 1:\n",
    "        nGramResultsSuccess = dfOut.iloc[[0,1,2,3,4],[0,1]]\n",
    "    else:    \n",
    "        nGramResultsSuccess = pd.concat([nGramResultsSuccess, dfOut.iloc[[0,1,2,3,4],[0,1]]], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Size Purpose ---')\n",
    "print(trainPurpose.shape[0])\n",
    "print('--- Size Success ---')\n",
    "print(trainSuccess.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGramResultsPurpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGramResultsSuccess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
